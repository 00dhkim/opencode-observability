# Gemini CLI Observability Hands On

[í•œêµ­ì–´ README](README_ko.md)

A local development environment that sends Gemini CLI requests to a LiteLLM Proxy, which then forwards traffic to the Google Gemini API.
Request/response logs can be viewed in Phoenix.

## ğŸš€ Components

* **LiteLLM Proxy**: OpenAI-compatible proxy that routes requests to Google Gemini.
* **Phoenix**: LLM observability UI.
* **Postgres**: Stores proxy metadata.
* **gemini-cli**: CLI used by developers.

## ğŸ“ Key Files

* `docker-compose.yml` â€“ Runs Proxy / Phoenix / Postgres
* `litellm_config.yaml` â€“ Defines model aliases and actual Gemini model IDs
* `.env` â€“ Google API Key + Proxy Master Key
* `env.sh` â€“ Environment variables for gemini-cli

## â–¶ï¸ How to Run

### 1) Generate an API Key from Google AI Studio

* Go to [https://aistudio.google.com/api-keys](https://aistudio.google.com/api-keys)
* Insert the key into `GEMINI_API_KEY` in `.env`

### 2) Start Containers

```bash
docker compose up -d
```

### 3) Apply gemini-cli Proxy Environment

```bash
source env.sh
```

### 4) Test

```bash
gemini
# Enter /auth, type sk-1234567890, then exit
gemini --model="gemini-2.5-flash-lite" -p "hello"
```

## ğŸ” Observability

Phoenix UI:
ğŸ‘‰ [http://localhost:6006](http://localhost:6006)

## ğŸ§ª Test Calling the Proxy Directly

```bash
curl -X POST "http://localhost:4000/v1beta/models/gemini-2.5-flash-lite:generateContent" \
  -H "Authorization: Bearer sk-1234567890" \
  -H "Content-Type: application/json" \
  -d '{"contents":[{"parts":[{"text":"hi"}],"role":"user"}]}'
```

## â— Troubleshooting

* If the gemini-cli model name â‰  `model_name`, a 404/500 may occur.
* Ensure `GEMINI_API_KEY` in `.env` is a valid Google API key.
* Check LiteLLM logs:

  ```bash
  docker logs -f litellm
  ```
